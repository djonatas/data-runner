# Data-Runner

üéØ **Executor de consultas/processos parametrizado por JSON**

Data-Runner √© uma ferramenta CLI robusta para **orquestra√ß√£o de pipelines de dados** que automatiza a execu√ß√£o de consultas SQL, transforma√ß√µes e carregamentos de dados entre diferentes fontes.

**O que o Data-Runner faz:**

- üîÑ **Automatiza pipelines de dados** atrav√©s de arquivos JSON de configura√ß√£o
- üóÑÔ∏è **Conecta m√∫ltiplas fontes** (PostgreSQL, MySQL, SQL Server, Oracle, CSV, SQLite)
- üìä **Executa consultas SQL** parametrizadas com vari√°veis din√¢micas
- üèóÔ∏è **Constr√≥i data warehouses** locais usando DuckDB como reposit√≥rio central
- üîó **Gerencia depend√™ncias** entre jobs para execu√ß√£o ordenada e paralela
- üìà **Monitora execu√ß√µes** com auditoria completa e hist√≥rico detalhado
- üéØ **Exporta resultados** para CSV com configura√ß√µes personaliz√°veis
- ‚ö° **Executa em lote** jobs individuais, por tipo ou grupos configurados

**Casos de uso t√≠picos:**

- Migra√ß√£o de dados entre sistemas
- ETL/ELT automatizado para data warehouses
- Consolida√ß√£o de dados de m√∫ltiplas fontes
- Relat√≥rios automatizados com exporta√ß√£o
- Valida√ß√£o e batimento de dados
- Pipelines de dados para an√°lise e BI

## üöÄ Quick Start

### Instala√ß√£o R√°pida

```bash
# Clone o reposit√≥rio
git clone https://github.com/djonatas/data-runner.git
cd data-runner

# Instala√ß√£o autom√°tica
./install.sh

# Ou instala√ß√£o manual
python -m pip install -e .
```

### Configura√ß√£o Inicial

```bash
# Setup autom√°tico
./setup.sh

# Ou configura√ß√£o manual
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json
```

### Uso B√°sico

```bash
# Listar jobs dispon√≠veis
data-runner list-jobs

# Executar um job
data-runner run --id meu_job

# Executar m√∫ltiplos jobs
data-runner run-batch --ids job1,job2,job3

# Ver hist√≥rico
data-runner history

# Ajuda
data-runner --help
```

## üìã Comandos CLI

### Listar Jobs

```bash
# Listar todos os jobs
data-runner list-jobs

# Listar grupos configurados
data-runner list-groups
```

### Executar Jobs

```bash
# Executar job √∫nico
data-runner run --id meu_job

# Executar job com limite
data-runner run --id meu_job --limit 1000

# Executar job em modo dry-run
data-runner run --id meu_job --dry-run

# Executar m√∫ltiplos jobs
data-runner run-batch --ids job1,job2,job3

# Executar jobs por tipo
data-runner run-group --type carga

# Executar grupo configurado
data-runner run-group-config --group meu_grupo
```

### Hist√≥rico e Inspe√ß√£o

```bash
# Ver hist√≥rico de execu√ß√µes
data-runner history

# Ver hist√≥rico de job espec√≠fico
data-runner history --query-id meu_job

# Inspecionar banco DuckDB
data-runner inspect

# Inspecionar tabela espec√≠fica
data-runner inspect --table minha_tabela
```

### Gerenciamento

```bash
# Remover tabela
data-runner drop-table --table tabela_antiga

# Remover tabela com confirma√ß√£o
data-runner drop-table --table tabela_antiga --confirm
```

## ‚öôÔ∏è Configura√ß√£o

### Arquivo connections.json

```json
{
  "defaultDuckDbPath": "./data/warehouse.duckdb",
  "connections": [
    {
      "name": "meu_postgres",
      "type": "postgres",
      "params": {
        "host": "${env:POSTGRES_HOST}",
        "port": 5432,
        "database": "${env:POSTGRES_DATABASE}",
        "user": "${env:POSTGRES_USER}",
        "password": "${env:POSTGRES_PASSWORD}",
        "schema": "public"
      }
    }
  ]
}
```

### Arquivo jobs.json

```json
{
  "variables": {
    "tenant_id": {
      "value": "12345",
      "type": "string",
      "description": "ID do tenant"
    }
  },
  "jobs": [
    {
      "queryId": "meu_job",
      "type": "carga",
      "connection": "meu_postgres",
      "sql": "SELECT * FROM usuarios WHERE tenant_id = '${var:tenant_id}'",
      "targetTable": "usuarios_importados",
      "dependencies": ["job_anterior"]
    }
  ],
  "job_groups": {
    "cargas_diarias": {
      "description": "Cargas di√°rias de dados",
      "job_ids": ["job1", "job2", "job3"]
    }
  }
}
```

### Arquivo .env

```bash
# Vari√°veis de ambiente
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=meu_banco
POSTGRES_USER=meu_usuario
POSTGRES_PASSWORD=minha_senha
```

## üîß Tipos de Conex√£o

### PostgreSQL

```json
{
  "name": "postgres_db",
  "type": "postgres",
  "params": {
    "host": "localhost",
    "port": 5432,
    "database": "meu_banco",
    "user": "usuario",
    "password": "senha",
    "schema": "public"
  }
}
```

### MySQL

```json
{
  "name": "mysql_db",
  "type": "mysql",
  "params": {
    "host": "localhost",
    "port": 3306,
    "database": "meu_banco",
    "user": "usuario",
    "password": "senha",
    "schema": "meu_schema"
  }
}
```

### SQL Server

```json
{
  "name": "mssql_db",
  "type": "mssql",
  "params": {
    "host": "localhost",
    "port": 1433,
    "database": "meu_banco",
    "user": "usuario",
    "password": "senha",
    "schema": "dbo"
  }
}
```

### Oracle

```json
{
  "name": "oracle_db",
  "type": "oracle",
  "params": {
    "host": "localhost",
    "port": 1521,
    "database": "XE",
    "user": "usuario",
    "password": "senha",
    "schema": "HR"
  }
}
```

### Oracle TNS Names

```json
{
  "name": "oracle_tns",
  "type": "oracle",
  "params": {
    "database": "ERP_PROD",
    "user": "usuario",
    "password": "senha",
    "schema": "HR"
  }
}
```

### CSV

```json
{
  "name": "csv_data",
  "type": "csv",
  "params": {
    "csv_file": "./data/arquivo.csv",
    "csv_separator": ",",
    "csv_encoding": "utf-8",
    "csv_has_header": true
  }
}
```

## üìä Tipos de Job

### Carga (carga)

```json
{
  "queryId": "importar_usuarios",
  "type": "carga",
  "connection": "postgres_db",
  "sql": "SELECT * FROM usuarios WHERE ativo = true",
  "targetTable": "usuarios_ativos"
}
```

### Batimento (batimento)

```json
{
  "queryId": "validar_dados",
  "type": "batimento",
  "connection": "postgres_db",
  "sql": "SELECT COUNT(*) as total FROM usuarios_ativos",
  "targetTable": "contagem_usuarios"
}
```

### Valida√ß√£o (validation)

```json
{
  "queryId": "validar_dados_usuarios",
  "type": "validation",
  "main_query": "importar_usuarios",
  "validation_file": "user_data_validation.py",
  "dependencies": ["importar_usuarios"]
}
```

### Export CSV (export-csv)

```json
{
  "queryId": "exportar_relatorio",
  "type": "export-csv",
  "connection": "postgres_db",
  "sql": "SELECT * FROM usuarios_ativos ORDER BY nome",
  "csv_file": "relatorio_usuarios.csv",
  "csv_separator": ",",
  "csv_encoding": "utf-8",
  "csv_include_header": true
}
```

## üîç Motor de Valida√ß√£o de Dados

O Data-Runner inclui um motor de valida√ß√£o que permite executar valida√ß√µes personalizadas em Python sobre os dados carregados.

### Como Funciona

1. **Configura√ß√£o**: Define um job do tipo `validation` no `jobs.json`
2. **Query Principal**: Especifica qual job (`main_query`) fornecer√° os dados
3. **Arquivo Python**: Cria um arquivo Python com a l√≥gica de valida√ß√£o
4. **Execu√ß√£o**: O motor carrega dinamicamente o arquivo e executa a valida√ß√£o

### Tipos de Valida√ß√£o

#### üîç Valida√ß√£o por Registro (Recomendado)

- **Fun√ß√£o**: `validate_record(record, context)`
- **Execu√ß√£o**: Uma vez para cada linha/registro dos dados
- **Uso**: Valida√ß√µes espec√≠ficas por registro, verifica√ß√µes individuais
- **Vantagem**: Detalhamento por registro, identifica√ß√£o precisa de problemas
- **Exemplo**: Validar email de cada usu√°rio individualmente

#### üìä Valida√ß√£o por Dataset (Tradicional)

- **Fun√ß√£o**: `validate(data, context)`
- **Execu√ß√£o**: Uma vez para todo o dataset
- **Uso**: Valida√ß√µes gerais, estat√≠sticas do dataset
- **Vantagem**: Vis√£o geral, valida√ß√µes de integridade
- **Exemplo**: Verificar se h√° duplicatas no dataset completo

### Estrutura do Arquivo de Valida√ß√£o

#### Valida√ß√£o por Registro (Recomendado)

```python
# validations/minha_validacao.py
from app.validation_engine import ValidationResult
import pandas as pd

def validate_record(record: Dict[str, Any], context: Dict[str, Any] = None) -> ValidationResult:
    """
    Fun√ß√£o executada uma vez para cada registro

    Args:
        record: Dicion√°rio com os dados do registro (inclui _record_index)
        context: Contexto adicional (main_query_id, validation_query_id, etc.)

    Returns:
        ValidationResult com o resultado da valida√ß√£o para este registro
    """

    record_index = record.get('_record_index', 'N/A')

    # Sua l√≥gica de valida√ß√£o para este registro espec√≠fico
    if 'id' not in record:
        return ValidationResult(
            success=False,
            message=f"Registro {record_index}: Campo 'id' obrigat√≥rio",
            details={"record_index": record_index, "missing_field": "id"}
        )

    if not record.get('name'):
        return ValidationResult(
            success=False,
            message=f"Registro {record_index}: Campo 'name' obrigat√≥rio",
            details={"record_index": record_index, "missing_field": "name"}
        )

    return ValidationResult(
        success=True,
        message=f"Registro {record_index} v√°lido",
        details={"record_index": record_index}
    )

# Fun√ß√£o de valida√ß√£o tradicional (opcional, para compatibilidade)
def validate(data: pd.DataFrame, context: Dict[str, Any] = None) -> ValidationResult:
    """
    Valida√ß√£o tradicional (executada uma vez para todo o dataset)
    """
    if data.empty:
        return ValidationResult(
            success=False,
            message="Nenhum dado encontrado",
            details={"row_count": 0}
        )

    return ValidationResult(
        success=True,
        message="Valida√ß√£o passou com sucesso",
        details={"row_count": len(data)}
    )
```

#### Valida√ß√£o Tradicional (Dataset Completo)

```python
# validations/validacao_tradicional.py
from app.validation_engine import ValidationResult
import pandas as pd

def validate(data: pd.DataFrame, context: Dict[str, Any] = None) -> ValidationResult:
    """
    Fun√ß√£o de valida√ß√£o tradicional (executada uma vez para todo o dataset)

    Args:
        data: DataFrame com os dados a serem validados
        context: Contexto adicional (main_query_id, validation_query_id, etc.)

    Returns:
        ValidationResult com o resultado da valida√ß√£o
    """

    # Sua l√≥gica de valida√ß√£o aqui
    if data.empty:
        return ValidationResult(
            success=False,
            message="Nenhum dado encontrado",
            details={"row_count": 0}
        )

    # Exemplo: verificar se h√° dados nulos
    null_count = data.isnull().sum().sum()

    if null_count > 0:
        return ValidationResult(
            success=False,
            message=f"Encontrados {null_count} valores nulos",
            details={"null_count": int(null_count)}
        )

    return ValidationResult(
        success=True,
        message="Valida√ß√£o passou com sucesso",
        details={"row_count": len(data)}
    )
```

### Par√¢metros de Valida√ß√£o

| Par√¢metro         | Tipo   | Obrigat√≥rio | Descri√ß√£o                                     |
| ----------------- | ------ | ----------- | --------------------------------------------- |
| `validation_file` | string | Sim         | Caminho do arquivo Python de valida√ß√£o        |
| `main_query`      | string | Sim         | ID do job que fornece os dados para valida√ß√£o |
| `connection`      | string | N√£o         | Conex√£o para contexto (opcional)              |
| `dependencies`    | array  | N√£o         | Lista de jobs que devem executar antes        |
| `output_table`    | string | N√£o         | Nome da tabela para salvar resultados         |
| `pkey_field`      | string | N√£o         | Campo chave prim√°ria para indexa√ß√£o           |

### Exemplos Pr√°ticos

#### Valida√ß√£o por Registro de Usu√°rios (Com Output)

```json
{
  "queryId": "validate_users_per_record",
  "type": "validation",
  "main_query": "load_users",
  "validation_file": "user_per_record_validation.py",
  "output_table": "user_validation_results",
  "pkey_field": "id",
  "dependencies": ["load_users"]
}
```

#### Valida√ß√£o por Registro (Sem Output)

```json
{
  "queryId": "validate_users_per_record_simple",
  "type": "validation",
  "main_query": "load_users",
  "validation_file": "user_per_record_validation.py",
  "dependencies": ["load_users"]
}
```

#### Valida√ß√£o Tradicional de Dataset

```json
{
  "queryId": "validate_sales_data",
  "type": "validation",
  "main_query": "load_sales_csv",
  "validation_file": "example_validation.py",
  "dependencies": ["load_sales_csv"]
}
```

#### Valida√ß√£o Individual de Registros

```json
{
  "queryId": "validate_records_individually",
  "type": "validation",
  "main_query": "load_products_csv",
  "validation_file": "per_record_validation.py",
  "dependencies": ["load_products_csv"]
}
```

### Execu√ß√£o de Valida√ß√µes

```bash
# Executar valida√ß√£o individual
data-runner run --id validate_user_data

# Executar todas as valida√ß√µes
data-runner run-group --type validation

# Executar grupo de valida√ß√µes
data-runner run-group-config --group validations
```

### Barra de Progresso

Durante a execu√ß√£o de valida√ß√µes por registro, o sistema exibe uma barra de progresso em tempo real:

```
üöÄ Validando 100 registros ‚Üí user_validation_results
============================================================
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% (100/100) | Tempo: 3.2s | 31.3 reg/s
‚úÖ Valida√ß√£o conclu√≠da!
   üìä Total processado: 100 registros
   ‚úÖ Sucessos: 95 (95.0%)
   ‚ùå Erros: 5
   ‚è±Ô∏è  Tempo total: 3.2s
   üöÄ Velocidade: 31.3 registros/segundo
   üíæ Resultados salvos em: user_validation_results
============================================================
```

#### Informa√ß√µes da Barra de Progresso

- **üìä Porcentagem**: Progresso atual da valida√ß√£o
- **üìà Barra visual**: Representa√ß√£o gr√°fica do progresso
- **üî¢ Contadores**: Registros processados vs. total
- **‚è±Ô∏è Tempo decorrido**: Tempo desde o in√≠cio
- **üéØ ETA**: Estimativa de tempo restante
- **üöÄ Velocidade**: Registros processados por segundo
- **‚úÖ/‚ùå Resultados**: Contadores de sucessos e erros
- **üíæ Output**: Tabela onde os resultados foram salvos

### Tabela de Output de Valida√ß√£o

Quando configurado com `output_table` e `pkey_field`, os resultados s√£o salvos em uma tabela estruturada:

#### Estrutura da Tabela

| Campo             | Tipo    | Descri√ß√£o                                |
| ----------------- | ------- | ---------------------------------------- |
| `execution_count` | INTEGER | N√∫mero sequencial da execu√ß√£o (PK)       |
| `pkey`            | VARCHAR | Chave prim√°ria do registro validado (PK) |
| `result`          | VARCHAR | Resultado: "success" ou "error"          |
| `message`         | TEXT    | Mensagem da valida√ß√£o                    |
| `details`         | TEXT    | Detalhes em JSON                         |
| `input_data`      | TEXT    | Dados do registro em JSON                |
| `executed_at`     | VARCHAR | Timestamp da execu√ß√£o                    |

#### Benef√≠cios da Tabela de Output

- **üìä Hist√≥rico completo**: Todas as valida√ß√µes por registro
- **üîç Rastreabilidade**: Identificar exatamente qual registro falhou
- **üìà An√°lise temporal**: Evolu√ß√£o da qualidade dos dados
- **üéØ Corre√ß√£o direcionada**: Saber exatamente o que corrigir
- **üìã Relat√≥rios**: Consultas SQL para an√°lise de qualidade

### Resultados de Valida√ß√£o

Os resultados s√£o armazenados na tabela de auditoria e incluem:

#### Para Valida√ß√£o por Registro:

- **Status**: Sucesso/Falha geral da valida√ß√£o
- **Mensagem**: Resumo dos resultados (ex: "150 de 200 registros v√°lidos")
- **Detalhes**: Estat√≠sticas detalhadas:
  - `validation_type`: "per_record"
  - `total_records`: N√∫mero total de registros
  - `successful_records`: Registros que passaram na valida√ß√£o
  - `failed_records`: Registros que falharam
  - `success_rate`: Taxa de sucesso em percentual
  - `failed_records_details`: Detalhes dos primeiros 10 registros que falharam
  - `error_records_details`: Detalhes dos primeiros 10 registros com erro

#### Para Valida√ß√£o Tradicional:

- **Status**: Sucesso/Falha da valida√ß√£o
- **Mensagem**: Descri√ß√£o do resultado
- **Detalhes**: Informa√ß√µes gerais sobre o dataset
- **Contexto**: Metadados sobre a execu√ß√£o

### Exemplos de Valida√ß√µes Inclu√≠das

#### Valida√ß√£o Tradicional (Dataset):

- **`example_validation.py`**: Valida√ß√£o gen√©rica com verifica√ß√µes b√°sicas
- **`user_data_validation.py`**: Valida√ß√£o espec√≠fica para dados de usu√°rios (email, telefone, CPF)

#### Valida√ß√£o por Registro:

- **`per_record_validation.py`**: Valida√ß√£o gen√©rica por registro (ID, nome, email, status)
- **`user_per_record_validation.py`**: Valida√ß√£o espec√≠fica de usu√°rios por registro (email, telefone, CPF)

## üîÑ Sistema de Depend√™ncias

```json
{
  "jobs": [
    {
      "queryId": "carregar_dados",
      "type": "carga",
      "connection": "postgres_db",
      "sql": "SELECT * FROM dados",
      "targetTable": "dados_carregados"
    },
    {
      "queryId": "processar_dados",
      "type": "carga",
      "connection": "postgres_db",
      "sql": "SELECT * FROM dados_carregados WHERE status = 'ativo'",
      "targetTable": "dados_processados",
      "dependencies": ["carregar_dados"]
    }
  ]
}
```

## üìÅ Grupos de Jobs

```json
{
  "job_groups": {
    "pipeline_completo": {
      "description": "Pipeline completo de dados",
      "job_ids": ["carregar_dados", "processar_dados", "exportar_relatorio"]
    },
    "cargas_diarias": {
      "description": "Cargas di√°rias",
      "job_ids": ["carregar_dados", "processar_dados"]
    }
  }
}
```

## üîê Vari√°veis de Ambiente

### Uso em Conex√µes

```json
{
  "params": {
    "host": "${env:POSTGRES_HOST}",
    "password": "${env:POSTGRES_PASSWORD}"
  }
}
```

### Uso em Jobs

```json
{
  "variables": {
    "tenant_id": {
      "value": "${env:TENANT_ID}",
      "type": "string"
    }
  }
}
```

## üõ†Ô∏è Scripts de Ajuda

### install.sh

```bash
# Instala√ß√£o completa
./install.sh
```

### setup.sh

```bash
# Setup e configura√ß√£o
./setup.sh
```

### run.sh

```bash
# Execu√ß√£o interativa
./run.sh

# Execu√ß√£o direta
./run.sh run meu_job
./run.sh batch job1,job2
./run.sh group carga
./run.sh group-config meu_grupo
```

### test.sh

```bash
# Testes e valida√ß√£o
./test.sh
```

## üìö Exemplos de Uso

### Execu√ß√£o B√°sica

```bash
# Listar jobs
data-runner list-jobs

# Executar job
data-runner run --id importar_usuarios

# Ver resultado
data-runner inspect --table usuarios_importados
```

### Execu√ß√£o com Limites

```bash
# Executar com limite de linhas
data-runner run --id importar_usuarios --limit 1000

# Executar em modo dry-run
data-runner run --id importar_usuarios --dry-run
```

### Execu√ß√£o em Lote

```bash
# Executar m√∫ltiplos jobs
data-runner run-batch --ids importar_usuarios,processar_dados,exportar_relatorio

# Executar por tipo
data-runner run-group --type carga

# Executar grupo configurado
data-runner run-group-config --group pipeline_completo
```

### Monitoramento

```bash
# Ver hist√≥rico
data-runner history

# Ver hist√≥rico de job espec√≠fico
data-runner history --query-id importar_usuarios

# Ver detalhes de tabela
data-runner inspect --table usuarios_importados
```

## üîç Troubleshooting

### Erro de Conex√£o

```bash
# Verificar configura√ß√µes
data-runner list-jobs

# Testar conex√£o
./test.sh config
```

### Erro de SQL

```bash
# Executar em modo dry-run
data-runner run --id problema_job --dry-run

# Verificar logs
python -m app run --id problema_job --verbose
```

### Problemas com DuckDB

```bash
# Inspecionar banco
data-runner inspect

# Ver tabelas
data-runner inspect --table audit_job_runs
```

---

## üë®‚Äçüíª Para Desenvolvedores

### Tecnologias Utilizadas

- **Python 3.8+**: Linguagem principal
- **DuckDB**: Banco de dados local para persist√™ncia
- **pandas**: Manipula√ß√£o de dados
- **Click**: Interface CLI
- **psycopg2**: Conex√£o PostgreSQL
- **PyMySQL**: Conex√£o MySQL
- **pyodbc**: Conex√£o SQL Server
- **cx_Oracle**: Conex√£o Oracle
- **python-dotenv**: Carregamento de vari√°veis de ambiente

### Estrutura do Projeto

```
data-runner/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # Interface CLI
‚îÇ   ‚îú‚îÄ‚îÄ runner.py           # Orquestrador principal
‚îÇ   ‚îú‚îÄ‚îÄ connections.py      # Gerenciamento de conex√µes
‚îÇ   ‚îú‚îÄ‚îÄ repository.py       # Persist√™ncia DuckDB
‚îÇ   ‚îú‚îÄ‚îÄ types.py           # Tipos e contratos
‚îÇ   ‚îú‚îÄ‚îÄ env_processor.py   # Processamento de vari√°veis de ambiente
‚îÇ   ‚îú‚îÄ‚îÄ variable_processor.py # Processamento de vari√°veis
‚îÇ   ‚îú‚îÄ‚îÄ dependency_manager.py # Gerenciamento de depend√™ncias
‚îÇ   ‚îî‚îÄ‚îÄ sql_utils.py       # Utilit√°rios SQL
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ connections.json.example
‚îÇ   ‚îî‚îÄ‚îÄ jobs.json.example
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ warehouse.duckdb
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ install.sh
    ‚îú‚îÄ‚îÄ setup.sh
    ‚îú‚îÄ‚îÄ run.sh
    ‚îî‚îÄ‚îÄ test.sh
```

### Fluxo Principal

1. **CLI** (`cli.py`) recebe comandos do usu√°rio
2. **Runner** (`runner.py`) orquestra a execu√ß√£o
3. **ConnectionFactory** (`connections.py`) cria conex√µes com bancos
4. **Repository** (`repository.py`) persiste resultados no DuckDB
5. **Processadores** lidam com vari√°veis e depend√™ncias

### Classes Principais

- **`JobRunner`**: Orquestrador principal, gerencia execu√ß√£o de jobs
- **`ConnectionFactory`**: Factory para criar conex√µes com diferentes bancos
- **`DuckDBRepository`**: Gerencia persist√™ncia e auditoria no DuckDB
- **`EnvironmentVariableProcessor`**: Processa vari√°veis de ambiente
- **`VariableProcessor`**: Processa vari√°veis definidas em jobs.json
- **`DependencyManager`**: Gerencia depend√™ncias e ordena√ß√£o de jobs

### Setup Local

```bash
# Clone e setup
git clone https://github.com/djonatas/data-runner.git
cd data-runner

# Ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Depend√™ncias
pip install -e .

# Depend√™ncias opcionais
pip install -e ".[mysql,mssql,oracle]"

# Configura√ß√£o
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json

# Teste
python -m app.cli --help
```

### Desenvolvimento

```bash
# Executar testes
python -m pytest tests/

# Executar CLI
python -m app.cli list-jobs

# Debug
python -m app.cli run --id teste --verbose
```

---

## üìÑ Licen√ßa

MIT License - veja arquivo LICENSE para detalhes.

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìû Suporte

- **Issues**: [GitHub Issues](https://github.com/djonatas/data-runner/issues)
