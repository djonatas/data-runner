# Data-Runner

üéØ **Executor de consultas/processos parametrizado por JSON**

Data-Runner √© uma ferramenta Python avan√ßada que permite executar consultas SQL em diferentes bancos de dados de forma parametrizada atrav√©s de arquivos JSON de configura√ß√£o. Com suporte a vari√°veis configur√°veis, sistema de depend√™ncias entre jobs, conex√µes Oracle via TNS Names, leitura de arquivos CSV e muito mais. Os resultados s√£o persistidos em um banco DuckDB local para an√°lise posterior.

## ‚ú® Caracter√≠sticas

- **Configura√ß√£o via JSON**: Defina conex√µes e jobs atrav√©s de arquivos JSON simples
- **M√∫ltiplos bancos suportados**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- **Persist√™ncia DuckDB**: Todos os resultados salvos localmente para an√°lise
- **Tipos de job**: Carga (dados) e Batimento (valida√ß√£o)
- **Auditoria completa**: Rastreamento de todas as execu√ß√µes
- **CLI intuitiva**: Interface de linha de comando f√°cil de usar
- **SQL parametrizado**: Suporte a vari√°veis de ambiente no SQL
- **Sistema de Vari√°veis**: Vari√°veis configur√°veis com tipos (string, number, boolean)
- **Sistema de Depend√™ncias**: Jobs com depend√™ncias e execu√ß√£o ordenada
- **Conex√µes CSV**: Leitura direta de arquivos CSV
- **Oracle TNS Names**: Suporte a conex√µes Oracle via TNS Names
- **Suporte a Schema**: Configura√ß√£o de schema padr√£o por conex√£o
- **Valida√ß√£o de Ciclos**: Detec√ß√£o autom√°tica de depend√™ncias circulares
- **Execu√ß√£o Paralela**: Jobs independentes executados em paralelo
- **Test√°vel e idempotente**: Execu√ß√µes seguras e repet√≠veis

## üöÄ Funcionalidades Principais

### üîó Sistema de Conex√µes

- **6 tipos de conex√£o**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- **Oracle TNS Names**: Suporte completo a conex√µes Oracle via TNS Names
- **Schema configur√°vel**: Defina schema padr√£o por conex√£o
- **Teste de conex√£o**: Valida√ß√£o autom√°tica de conex√µes

### üìä Sistema de Vari√°veis

- **Tipos suportados**: String, Number, Boolean
- **Substitui√ß√£o autom√°tica**: Vari√°veis processadas em tempo de execu√ß√£o
- **Valida√ß√£o de tipos**: Verifica√ß√£o autom√°tica de tipos de dados
- **Sintaxe simples**: Use `${var:nome_da_variavel}` em suas queries

### üîÑ Sistema de Depend√™ncias

- **Depend√™ncias entre jobs**: Configure jobs que dependem de outros
- **Ordena√ß√£o autom√°tica**: Execu√ß√£o na ordem correta baseada em depend√™ncias
- **Detec√ß√£o de ciclos**: Valida√ß√£o autom√°tica de depend√™ncias circulares
- **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente

### üìÅ Conex√µes CSV

- **Leitura direta**: Carregue dados de arquivos CSV sem SQL
- **Configura√ß√£o flex√≠vel**: Separador, encoding, header configur√°veis
- **Integra√ß√£o completa**: Funciona com sistema de depend√™ncias e vari√°veis

### üõ°Ô∏è Seguran√ßa e Configura√ß√£o

- **Arquivos de exemplo**: Templates seguros para configura√ß√£o
- **Gitignore autom√°tico**: Prote√ß√£o de dados sens√≠veis
- **Valida√ß√£o robusta**: Verifica√ß√£o de configura√ß√µes antes da execu√ß√£o

## üöÄ Instala√ß√£o

### Pr√©-requisitos

- Python 3.11 ou superior
- pip (gerenciador de pacotes Python)

### Instala√ß√£o b√°sica

```bash
# Clone o reposit√≥rio
git clone <repository-url>
cd data-runner

# Instale as depend√™ncias
pip install -e .

# Ou instale com depend√™ncias opcionais
pip install -e ".[all]"

# Configure os arquivos de configura√ß√£o
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json

# Edite os arquivos com suas configura√ß√µes reais
# config/connections.json - conex√µes de banco de dados
# config/jobs.json - jobs e queries
```

### Depend√™ncias opcionais

Para conectar com bancos espec√≠ficos, instale as depend√™ncias correspondentes:

```bash
# Para MySQL
pip install -e ".[mysql]"

# Para MSSQL
pip install -e ".[mssql]"

# Para Oracle
pip install -e ".[oracle]"

# Para desenvolvimento (inclui testes)
pip install -e ".[dev]"
```

## ‚ö° Quick Start

### üöÄ Instala√ß√£o R√°pida (Recomendado)

```bash
# Clone o reposit√≥rio
git clone <repository-url>
cd data-runner

# Instala√ß√£o autom√°tica com scripts
./install.sh
```

### üìú Scripts Shell Dispon√≠veis

O Data-Runner inclui scripts shell para facilitar o uso:

```bash
# Instala√ß√£o completa com depend√™ncias opcionais
./install.sh

# Setup r√°pido e configura√ß√£o
./setup.sh

# Execu√ß√£o de jobs (modo interativo)
./run.sh

# Execu√ß√£o direta
./run.sh run "meu_job"
./run.sh batch "job1,job2,job3"

# Testes e valida√ß√£o
./test.sh
```

### üîß Instala√ß√£o Manual

```bash
# Instale as depend√™ncias
pip install -e .

# Configure os arquivos
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json

# Edite com suas configura√ß√µes reais
```

### ‚ñ∂Ô∏è Execu√ß√£o de Jobs

#### üìã Listar Jobs Dispon√≠veis

```bash
# Via CLI
data-runner list-jobs

# Via script
./run.sh list
```

**Retorno esperado:**

```
üìã Jobs dispon√≠veis:
  - active_in_brm (carga)
  - person_id_duplicado (batimento)
  - products_daily_sync (carga)
  - sales_validation (batimento)
  - oracle_hr_data (carga)
  - oracle_tns_financial_data (carga)
  - load_products_csv (carga)
  - load_customers_csv (carga)
  - load_sales_csv (carga)
  - transform_products (carga)
  - transform_customers (carga)
  - create_sales_fact (carga)
  - create_daily_summary (carga)
  - create_product_summary (carga)
  - final_validation (batimento)
```

#### üöÄ Executar Job √önico

```bash
# Via CLI
data-runner run --id "active_in_brm"

# Via script
./run.sh run "active_in_brm"
```

**Retorno esperado:**

```
üöÄ Data-Runner - Executando Job
================================
üìã Job: active_in_brm
üîó Conex√£o: BRM_EXAMPLE (postgres)
üìä Tipo: carga
üéØ Tabela destino: stg_active_people

‚è≥ Executando query...
‚úÖ Query executada com sucesso!
üìà Resultados: 1,247 linhas processadas
üíæ Dados salvos em: stg_active_people

‚úÖ Job 'active_in_brm' executado com sucesso!
‚è±Ô∏è  Tempo total: 2.3s
```

#### üì¶ Executar M√∫ltiplos Jobs

```bash
# Via CLI
data-runner run-batch "load_products_csv,transform_products,create_sales_fact"

# Via script
./run.sh batch "load_products_csv,transform_products,create_sales_fact"
```

**Retorno esperado:**

```
üöÄ Data-Runner - Executando Pipeline
===================================
üìã Jobs: load_products_csv, transform_products, create_sales_fact
üîÑ Ordem de execu√ß√£o: load_products_csv ‚Üí transform_products ‚Üí create_sales_fact

‚è≥ Executando: load_products_csv...
‚úÖ Job 'load_products_csv' executado (1,500 linhas)

‚è≥ Executando: transform_products...
‚úÖ Job 'transform_products' executado (1,500 linhas)

‚è≥ Executando: create_sales_fact...
‚úÖ Job 'create_sales_fact' executado (3,200 linhas)

üéâ Pipeline executado com sucesso!
üìä Total: 6,200 linhas processadas
‚è±Ô∏è  Tempo total: 8.7s
```

#### üîç Executar com Dry-Run

```bash
# Via CLI
data-runner run --id "products_daily_sync" --dry-run

# Via script
./run.sh run "products_daily_sync" --dry-run
```

**Retorno esperado:**

```
üîç Data-Runner - Dry Run Mode
============================
üìã Job: products_daily_sync
üîó Conex√£o: mysql_analytics (mysql)
üìä Tipo: carga

üîç SQL que seria executado:
SELECT product_id, name, price, category, updated_at
FROM products
WHERE updated_at >= '2024-01-01'
  AND updated_at <= '2024-12-31'
  AND price >= 1000;

üìä Vari√°veis processadas:
  - start_date: '2024-01-01'
  - end_date: '2024-12-31'
  - min_amount: 1000

‚ö†Ô∏è  Modo dry-run - nenhum dado foi processado
```

#### üìä Executar com Limite

```bash
# Via CLI
data-runner run --id "oracle_hr_data" --limit 100

# Via script
./run.sh run "oracle_hr_data" --limit 100
```

**Retorno esperado:**

```
üöÄ Data-Runner - Executando Job (Limitado)
==========================================
üìã Job: oracle_hr_data
üîó Conex√£o: oracle_erp (oracle)
üìä Tipo: carga
üéØ Tabela destino: stg_oracle_employees
üî¢ Limite: 100 linhas

‚è≥ Executando query...
‚úÖ Query executada com sucesso!
üìà Resultados: 100 linhas processadas (limitado)
üíæ Dados salvos em: stg_oracle_employees

‚úÖ Job 'oracle_hr_data' executado com sucesso!
‚è±Ô∏è  Tempo total: 1.2s
```

#### üìà Ver Hist√≥rico de Execu√ß√µes

```bash
# Via CLI
data-runner history

# Via script
./run.sh history
```

**Retorno esperado:**

```
üìà Data-Runner - Hist√≥rico de Execu√ß√µes
======================================
üìÖ √öltimas 10 execu√ß√µes:

1. active_in_brm        | 2024-09-18 15:30:25 | SUCCESS | 1,247 linhas | 2.3s
2. load_products_csv    | 2024-09-18 15:28:10 | SUCCESS | 1,500 linhas | 1.8s
3. transform_products   | 2024-09-18 15:28:12 | SUCCESS | 1,500 linhas | 2.1s
4. create_sales_fact    | 2024-09-18 15:28:15 | SUCCESS | 3,200 linhas | 4.9s
5. sales_validation     | 2024-09-18 15:25:00 | SUCCESS | 45 linhas   | 0.8s
6. oracle_hr_data       | 2024-09-18 15:20:15 | SUCCESS | 2,100 linhas | 3.2s
7. person_id_duplicado  | 2024-09-18 15:18:30 | ERROR   | 0 linhas     | 0.5s
8. products_daily_sync  | 2024-09-18 15:15:45 | SUCCESS | 850 linhas   | 1.9s
9. load_customers_csv   | 2024-09-18 15:10:20 | SUCCESS | 3,500 linhas | 2.4s
10. final_validation    | 2024-09-18 15:05:10 | SUCCESS | 12 linhas    | 0.3s

üìä Estat√≠sticas:
  ‚úÖ Sucessos: 9/10 (90%)
  ‚ùå Erros: 1/10 (10%)
  üìà Total processado: 14,054 linhas
  ‚è±Ô∏è  Tempo m√©dio: 1.9s por job
```

#### üîç Inspecionar Banco DuckDB

```bash
# Via CLI
data-runner inspect

# Via script
./run.sh inspect
```

**Retorno esperado:**

```
üîç Data-Runner - Inspe√ß√£o do DuckDB
==================================
üìÅ Arquivo: ./data/warehouse.duckdb
üìä Tamanho: 4.2 MB

üìã Tabelas dispon√≠veis:

1. stg_active_people
   üìä Linhas: 1,247
   üìÖ √öltima atualiza√ß√£o: 2024-09-18 15:30:25
   üìù Colunas: person_id, name, email, active, created_at

2. stg_products
   üìä Linhas: 1,500
   üìÖ √öltima atualiza√ß√£o: 2024-09-18 15:28:10
   üìù Colunas: product_id, name, price, category, updated_at

3. stg_oracle_employees
   üìä Linhas: 2,100
   üìÖ √öltima atualiza√ß√£o: 2024-09-18 15:20:15
   üìù Colunas: employee_id, first_name, last_name, hire_date, department_id

4. fact_sales
   üìä Linhas: 3,200
   üìÖ √öltima atualiza√ß√£o: 2024-09-18 15:28:15
   üìù Colunas: sale_id, product_id, customer_id, amount, date

5. audit_job_runs
   üìä Linhas: 156
   üìÖ √öltima atualiza√ß√£o: 2024-09-18 15:30:25
   üìù Colunas: run_id, query_id, status, started_at, finished_at, rowcount

üíæ Total de dados: 8,203 linhas
üóÑÔ∏è  Espa√ßo usado: 4.2 MB
```

#### ‚ö†Ô∏è Exemplo de Erro

```bash
data-runner run --id "job_inexistente"
```

**Retorno esperado:**

```
‚ùå Data-Runner - Erro
====================
üìã Job: job_inexistente

‚ùå Erro: Job 'job_inexistente' n√£o encontrado

üí° Jobs dispon√≠veis:
  - active_in_brm
  - person_id_duplicado
  - products_daily_sync
  - sales_validation
  - oracle_hr_data
  - oracle_tns_financial_data
  - load_products_csv
  - load_customers_csv
  - load_sales_csv
  - transform_products
  - transform_customers
  - create_sales_fact
  - create_daily_summary
  - create_product_summary
  - final_validation

üîß Use 'data-runner list-jobs' para ver todos os jobs dispon√≠veis
```

#### üóëÔ∏è Remover Tabela

```bash
# Via CLI
data-runner drop-table --table "stg_products"

# Via script
./run.sh drop "stg_products"
```

**Retorno esperado:**

```
üóëÔ∏è Remo√ß√£o de Tabela
==================================================
Tabela: stg_products
Linhas: 1,500

‚ö†Ô∏è Tem certeza que deseja remover a tabela 'stg_products'? [y/N]: y

‚úÖ Tabela 'stg_products' removida com sucesso!
```

**Com confirma√ß√£o autom√°tica:**

```bash
# Via CLI com --confirm
data-runner drop-table --table "stg_products" --confirm

# Via script (pergunta confirma√ß√£o)
./run.sh drop "stg_products"
```

**Retorno esperado:**

```
üóëÔ∏è Remo√ß√£o de Tabela
==================================================
Tabela: stg_products
Linhas: 1,500

‚úÖ Tabela 'stg_products' removida com sucesso!
```

**Tentativa de remover tabela inexistente:**

```bash
data-runner drop-table --table "tabela_inexistente"
```

**Retorno esperado:**

```
‚ùå Tabela 'tabela_inexistente' n√£o encontrada
```

## üìÅ Estrutura do Projeto

```
data-runner/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                    # Interface de linha de comando
‚îÇ   ‚îú‚îÄ‚îÄ connections.py            # Gerenciamento de conex√µes
‚îÇ   ‚îú‚îÄ‚îÄ dependency_manager.py     # Gerenciamento de depend√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ repository.py             # Persist√™ncia DuckDB
‚îÇ   ‚îú‚îÄ‚îÄ runner.py                # Orquestrador principal
‚îÇ   ‚îú‚îÄ‚îÄ sql_utils.py             # Utilit√°rios SQL
‚îÇ   ‚îú‚îÄ‚îÄ types.py                 # Tipos e contratos
‚îÇ   ‚îî‚îÄ‚îÄ variable_processor.py    # Processamento de vari√°veis
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ connections.json.example  # Exemplo de conex√µes
‚îÇ   ‚îú‚îÄ‚îÄ jobs.json.example        # Exemplo de jobs
‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Documenta√ß√£o de configura√ß√£o
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ products.csv             # Dados de exemplo
‚îÇ   ‚îú‚îÄ‚îÄ customers.csv            # Dados de exemplo
‚îÇ   ‚îú‚îÄ‚îÄ sales_data.csv           # Dados de exemplo
‚îÇ   ‚îî‚îÄ‚îÄ warehouse.duckdb         # Banco DuckDB (criado automaticamente)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_config_parsing.py
‚îÇ   ‚îî‚îÄ‚îÄ test_runner_sqlite.py
‚îú‚îÄ‚îÄ install.sh                   # Script de instala√ß√£o completa
‚îú‚îÄ‚îÄ setup.sh                     # Script de setup r√°pido
‚îú‚îÄ‚îÄ run.sh                       # Script de execu√ß√£o de jobs
‚îú‚îÄ‚îÄ test.sh                      # Script de testes e valida√ß√£o
‚îú‚îÄ‚îÄ SCRIPTS.md                   # Documenta√ß√£o dos scripts
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

## ‚öôÔ∏è Configura√ß√£o

### Configura√ß√£o Inicial

1. **Copie os arquivos de exemplo:**

```bash
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json
```

2. **Edite as configura√ß√µes:**

   - `config/connections.json` - Configure suas conex√µes de banco
   - `config/jobs.json` - Configure seus jobs e queries

3. **Seguran√ßa:**
   - Os arquivos reais s√£o ignorados pelo Git
   - Use os arquivos `.example` como refer√™ncia
   - Nunca commite senhas ou dados sens√≠veis

### 1. Configura√ß√£o de Conex√µes (`config/connections.json`)

```json
{
  "defaultDuckDbPath": "./data/warehouse.duckdb",
  "connections": [
    {
      "name": "pg_main",
      "type": "postgres",
      "params": {
        "host": "localhost",
        "port": 5432,
        "database": "mydb",
        "user": "user",
        "password": "pass",
        "schema": "public"
      }
    },
    {
      "name": "mysql_analytics",
      "type": "mysql",
      "params": {
        "host": "localhost",
        "port": 3306,
        "database": "analytics",
        "user": "analytics_user",
        "password": "analytics_pass",
        "schema": "analytics"
      }
    },
    {
      "name": "mssql_warehouse",
      "type": "mssql",
      "params": {
        "host": "localhost",
        "port": 1433,
        "database": "warehouse",
        "user": "warehouse_user",
        "password": "warehouse_pass",
        "schema": "dbo"
      }
    },
    {
      "name": "sqlite_dev",
      "type": "sqlite",
      "params": {
        "filepath": "./data/dev.sqlite"
      }
    }
  ]
}
```

#### Suporte a Schema

O Data-Runner agora suporta configura√ß√£o de schema padr√£o para cada conex√£o:

- **PostgreSQL**: Use `"schema": "nome_do_schema"` para definir o search_path
- **MySQL**: Use `"schema": "nome_do_database"` para definir o database padr√£o
- **MSSQL**: Use `"schema": "nome_do_schema"` para definir o schema padr√£o
- **SQLite**: N√£o requer configura√ß√£o de schema (usa o arquivo diretamente)

O schema configurado ser√° aplicado automaticamente a todas as queries executadas na conex√£o.

### 2. Configura√ß√£o de Jobs (`config/jobs.json`)

```json
{
  "variables": {
    "start_date": {
      "value": "2024-01-01",
      "type": "string",
      "description": "Data de in√≠cio para filtros"
    },
    "min_amount": {
      "value": 100.0,
      "type": "number",
      "description": "Valor m√≠nimo"
    },
    "active_only": {
      "value": true,
      "type": "boolean",
      "description": "Filtrar apenas ativos"
    }
  },
  "jobs": [
    {
      "queryId": "orders_full_load",
      "type": "carga",
      "connection": "pg_main",
      "sql": "SELECT * FROM public.orders WHERE order_date >= '${var:start_date}' AND amount >= ${var:min_amount};",
      "targetTable": "stg_orders"
    },
    {
      "queryId": "customers_mismatch",
      "type": "batimento",
      "connection": "sqlite_dev",
      "sql": "SELECT c.id, c.email FROM customers c LEFT JOIN crm_customers crm ON crm.id = c.id WHERE crm.id IS NULL AND ${var:active_only} = true;"
    }
  ]
}
```

#### Sistema de Vari√°veis

O Data-Runner suporta vari√°veis configur√°veis que podem ser usadas em qualquer query:

**Tipos de Vari√°veis:**

- **`string`**: Texto (ser√° automaticamente envolvido em aspas simples)
- **`number`**: N√∫meros (inteiros ou decimais)
- **`boolean`**: Valores booleanos (true/false)

**Sintaxe de Uso:**

- Use `${var:nome_variavel}` nas suas queries SQL
- As vari√°veis s√£o substitu√≠das automaticamente durante a execu√ß√£o

**Exemplos:**

```sql
-- Vari√°vel string
SELECT * FROM users WHERE name = '${var:user_name}';

-- Vari√°vel num√©rica
SELECT * FROM orders WHERE amount >= ${var:min_amount};

-- Vari√°vel booleana
SELECT * FROM products WHERE active = ${var:active_only};
```

### Tipos de Conex√£o Suportados

- **PostgreSQL**: `"type": "postgres"`
- **SQLite**: `"type": "sqlite"`
- **MySQL**: `"type": "mysql"`
- **MSSQL**: `"type": "mssql"`
- **Oracle**: `"type": "oracle"`
- **CSV**: `"type": "csv"`

#### Conex√µes Oracle

O Data-Runner suporta conex√µes Oracle usando cx_Oracle com dois m√©todos:

##### 1. Conex√£o Direta (Host/Porta)

```json
{
  "name": "oracle_erp",
  "type": "oracle",
  "params": {
    "host": "oracle-server.example.com",
    "port": 1521,
    "database": "XE",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

##### 2. Conex√£o via TNS Names

```json
{
  "name": "oracle_tns_erp",
  "type": "oracle",
  "params": {
    "database": "ERP_PROD",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

**Par√¢metros Oracle:**

- `host`: Servidor Oracle (opcional para TNS Names)
- `port`: Porta do Oracle (opcional para TNS Names)
- `database`: Service Name, SID ou TNS Name do Oracle
- `user`: Usu√°rio Oracle
- `password`: Senha Oracle
- `schema`: Schema padr√£o (opcional)

**Conex√£o via TNS Names:**

- Configure o arquivo `tnsnames.ora` no cliente Oracle
- Use apenas `database` com o nome do TNS
- O `host` e `port` s√£o opcionais quando usando TNS Names

**Depend√™ncias:**

```bash
pip install cx_Oracle
```

#### Conex√µes CSV

O Data-Runner suporta leitura direta de arquivos CSV:

```json
{
  "name": "csv_products",
  "type": "csv",
  "params": {
    "csv_file": "./data/products.csv",
    "csv_separator": ",",
    "csv_encoding": "utf-8",
    "csv_has_header": true
  }
}
```

**Par√¢metros CSV:**

- `csv_file`: Caminho para o arquivo CSV (obrigat√≥rio)
- `csv_separator`: Separador usado no CSV (padr√£o: `,`)
- `csv_encoding`: Encoding do arquivo (padr√£o: `utf-8`)
- `csv_has_header`: Se o arquivo tem cabe√ßalho (padr√£o: `true`)

**Jobs CSV:**
Para conex√µes CSV, o campo `sql` √© opcional, pois o sistema l√™ o arquivo inteiro:

```json
{
  "queryId": "load_products_csv",
  "type": "carga",
  "connection": "csv_products",
  "targetTable": "stg_products"
}
```

#### Sistema de Depend√™ncias

O Data-Runner suporta depend√™ncias entre jobs para criar pipelines de dados:

```json
{
  "queryId": "transform_products",
  "type": "carga",
  "connection": "sqlite_dev",
  "sql": "SELECT * FROM stg_products WHERE active = 1;",
  "targetTable": "dim_products",
  "dependencies": ["load_products_csv"]
},
{
  "queryId": "create_sales_fact",
  "type": "carga",
  "connection": "sqlite_dev",
  "sql": "SELECT s.*, p.name FROM stg_sales s JOIN stg_products p ON s.product_id = p.id;",
  "targetTable": "fact_sales",
  "dependencies": ["load_sales_csv", "load_products_csv"]
}
```

**Funcionalidades:**

- **Ordena√ß√£o Autom√°tica**: Jobs s√£o executados na ordem correta das depend√™ncias
- **Valida√ß√£o de Ciclos**: Detecta depend√™ncias circulares
- **Execu√ß√£o Paralela**: Jobs sem depend√™ncias podem executar em paralelo
- **Resolu√ß√£o Inteligente**: Inclui automaticamente depend√™ncias necess√°rias

### Tipos de Job

- **Carga** (`"type": "carga"`): Carrega dados para tabela de staging
- **Batimento** (`"type": "batimento"`): Valida dados e salva diverg√™ncias

## üñ•Ô∏è Uso

### Comandos Principais

```bash
# Listar jobs dispon√≠veis
python -m app list

# Executar um job espec√≠fico
python -m app run --id orders_full_load

# Executar m√∫ltiplos jobs
python -m app run --ids orders_full_load,customers_mismatch

# Executar com op√ß√µes
python -m app run --id orders_full_load --limit 1000 --dry-run

# Ver hist√≥rico de execu√ß√µes
python -m app history

# Inspecionar tabelas no DuckDB
python -m app inspect
```

### Op√ß√µes de Execu√ß√£o

- `--duckdb <path>`: Caminho personalizado para o DuckDB
- `--dry-run`: Mostra o que faria sem executar
- `--limit <N>`: Limita o n√∫mero de linhas retornadas
- `--save-as <table>`: Nome personalizado para a tabela alvo

### Exemplos de Uso

```bash
# Execu√ß√£o simples
python -m app run --id orders_full_load

# Execu√ß√£o com limite e dry-run
python -m app run --id customers_mismatch --limit 100 --dry-run

# Execu√ß√£o em lote
python -m app run --ids orders_full_load,customers_mismatch,products_daily_sync

# Ver hist√≥rico de um job espec√≠fico
python -m app history --query-id orders_full_load

# Inspecionar tabela espec√≠fica
python -m app inspect --table stg_orders
```

## üóÑÔ∏è Estrutura do DuckDB

### Tabela de Auditoria (`audit_job_runs`)

| Coluna         | Tipo    | Descri√ß√£o                     |
| -------------- | ------- | ----------------------------- |
| `run_id`       | VARCHAR | ID √∫nico da execu√ß√£o (UUID)   |
| `query_id`     | VARCHAR | ID da query executada         |
| `type`         | VARCHAR | Tipo do job (carga/batimento) |
| `started_at`   | VARCHAR | Timestamp de in√≠cio           |
| `finished_at`  | VARCHAR | Timestamp de fim              |
| `status`       | VARCHAR | Status (success/error)        |
| `rowcount`     | INTEGER | N√∫mero de linhas processadas  |
| `error`        | VARCHAR | Mensagem de erro (se houver)  |
| `target_table` | VARCHAR | Tabela alvo                   |
| `connection`   | VARCHAR | Conex√£o utilizada             |

### Conven√ß√µes de Nomenclatura

- **Jobs de Carga**: `stg_<queryId>` (ex: `stg_orders`)
- **Jobs de Batimento**: `val_<queryId>` (ex: `val_customers_mismatch`)

## üîß SQL Parametrizado

### Vari√°veis de Ambiente

Use `${env:VAR_NAME}` no SQL para referenciar vari√°veis de ambiente:

```sql
SELECT '${env:USER}' as current_user,
       '${env:HOME}' as home_dir,
       datetime('now') as current_time;
```

### Aplica√ß√£o de LIMIT

O par√¢metro `--limit` √© aplicado automaticamente ao SQL:

```bash
python -m app run --id orders_full_load --limit 1000
# Adiciona "LIMIT 1000" ao final da query
```

## üß™ Testes

Execute os testes com pytest:

```bash
# Instalar depend√™ncias de desenvolvimento
pip install -e ".[dev]"

# Executar todos os testes
pytest

# Executar com cobertura
pytest --cov=app

# Executar testes espec√≠ficos
pytest tests/test_config_parsing.py
pytest tests/test_runner_sqlite.py
```

## üìù Boas Pr√°ticas

### Para Jobs de Carga

1. **Use filtros de data**: Sempre inclua filtros de data para evitar carregar dados desnecess√°rios
2. **Defina target_table**: Especifique explicitamente o nome da tabela alvo
3. **Teste com --dry-run**: Use `--dry-run` antes de executar jobs grandes
4. **Use LIMIT para testes**: Use `--limit` para testar com poucos dados

```json
{
  "queryId": "orders_daily_load",
  "type": "carga",
  "connection": "pg_main",
  "sql": "SELECT * FROM orders WHERE order_date = CURRENT_DATE;",
  "targetTable": "stg_orders_daily"
}
```

### Para Jobs de Batimento

1. **Identifique diverg√™ncias claramente**: Use LEFT JOIN para encontrar registros √≥rf√£os
2. **Inclua campos identificadores**: Sempre inclua IDs para facilitar corre√ß√£o
3. **Documente a regra**: Use coment√°rios no SQL para explicar a valida√ß√£o

```json
{
  "queryId": "customers_orphan_validation",
  "type": "batimento",
  "connection": "sqlite_dev",
  "sql": "-- Valida√ß√£o: Clientes sem pedidos nos √∫ltimos 30 dias\nSELECT c.id, c.name, c.email, c.created_at\nFROM customers c\nLEFT JOIN orders o ON o.customer_id = c.id AND o.order_date >= date('now', '-30 days')\nWHERE o.id IS NULL;"
}
```

### Para Configura√ß√£o de Conex√µes

1. **Use vari√°veis de ambiente**: Para senhas e dados sens√≠veis
2. **Teste conex√µes**: Use `python -m app list` para validar configura√ß√µes
3. **Organize por ambiente**: Diferentes conex√µes para dev/prod

```json
{
  "name": "pg_prod",
  "type": "postgres",
  "params": {
    "host": "${env:PG_HOST}",
    "port": 5432,
    "database": "${env:PG_DATABASE}",
    "user": "${env:PG_USER}",
    "password": "${env:PG_PASSWORD}"
  }
}
```

## üöÄ Funcionalidades Avan√ßadas

### üìä Sistema de Vari√°veis Configur√°veis

O Data-Runner permite definir vari√°veis personalizadas que podem ser usadas em qualquer query SQL:

```json
{
  "variables": {
    "start_date": {
      "value": "2024-01-01",
      "type": "string",
      "description": "Data de in√≠cio para filtros"
    },
    "min_amount": {
      "value": 1000,
      "type": "number",
      "description": "Valor m√≠nimo para vendas"
    },
    "active_only": {
      "value": true,
      "type": "boolean",
      "description": "Filtrar apenas registros ativos"
    }
  }
}
```

**Uso nas queries:**

```sql
SELECT * FROM sales
WHERE date >= ${var:start_date}
  AND amount >= ${var:min_amount}
  AND active = ${var:active_only};
```

### üîÑ Sistema de Depend√™ncias Entre Jobs

Configure jobs que dependem de outros para criar pipelines de dados:

```json
{
  "jobs": [
    {
      "queryId": "load_products_csv",
      "type": "carga",
      "connection": "csv_products",
      "targetTable": "staging_products"
    },
    {
      "queryId": "transform_products",
      "type": "carga",
      "connection": "warehouse",
      "sql": "SELECT * FROM staging_products WHERE price > 0",
      "targetTable": "dim_products",
      "dependencies": ["load_products_csv"]
    },
    {
      "queryId": "create_sales_fact",
      "type": "carga",
      "connection": "warehouse",
      "sql": "SELECT p.*, s.* FROM dim_products p JOIN sales s ON p.id = s.product_id",
      "targetTable": "fact_sales",
      "dependencies": ["load_products_csv", "load_customers_csv"]
    }
  ]
}
```

**Funcionalidades:**

- ‚úÖ **Ordena√ß√£o autom√°tica**: Jobs executados na ordem correta
- ‚úÖ **Detec√ß√£o de ciclos**: Valida√ß√£o de depend√™ncias circulares
- ‚úÖ **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente
- ‚úÖ **Valida√ß√£o robusta**: Verifica√ß√£o de depend√™ncias inexistentes

### üìÅ Conex√µes CSV Avan√ßadas

Leia arquivos CSV diretamente sem necessidade de SQL:

```json
{
  "name": "csv_products",
  "type": "csv",
  "params": {
    "csv_file": "./data/products.csv",
    "csv_separator": ",",
    "csv_encoding": "utf-8",
    "csv_has_header": true
  }
}
```

**Jobs CSV:**

```json
{
  "queryId": "load_products_csv",
  "type": "carga",
  "connection": "csv_products",
  "targetTable": "staging_products"
}
```

### üîó Oracle TNS Names

Suporte completo a conex√µes Oracle via TNS Names:

```json
{
  "name": "oracle_erp",
  "type": "oracle",
  "params": {
    "database": "ERP_PROD",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

**Configura√ß√£o TNS Names:**

```bash
# Arquivo tnsnames.ora
ERP_PROD =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = oracle-server.company.com)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = ERP_PROD)
    )
  )
```

### üèóÔ∏è Suporte a Schema

Configure schema padr√£o para diferentes bancos:

```json
{
  "name": "postgres_analytics",
  "type": "postgres",
  "params": {
    "host": "localhost",
    "port": 5432,
    "database": "analytics",
    "user": "analytics_user",
    "password": "analytics_pass",
    "schema": "analytics"
  }
}
```

**Bancos suportados:**

- **PostgreSQL**: `search_path` configurado automaticamente
- **MySQL**: `USE schema` executado automaticamente
- **MSSQL**: `USE schema` executado automaticamente
- **Oracle**: `ALTER SESSION SET CURRENT_SCHEMA` executado automaticamente

## üìú Scripts Shell

O Data-Runner inclui scripts shell para facilitar o uso e automa√ß√£o:

### üöÄ `install.sh` - Instala√ß√£o Completa

```bash
./install.sh
```

- ‚úÖ Verifica Python 3.11+
- ‚úÖ Cria ambiente virtual
- ‚úÖ Instala depend√™ncias b√°sicas e opcionais
- ‚úÖ Configura arquivos de exemplo
- ‚úÖ Interface colorida e interativa

### ‚ö° `setup.sh` - Setup R√°pido

```bash
./setup.sh
```

- ‚úÖ Configura√ß√£o r√°pida para desenvolvimento
- ‚úÖ Menu interativo para configura√ß√£o
- ‚úÖ Backups autom√°ticos
- ‚úÖ Teste de configura√ß√£o

### ‚ñ∂Ô∏è `run.sh` - Executor de Jobs

```bash
# Modo interativo
./run.sh

# Modo direto
./run.sh run "meu_job"
./run.sh batch "job1,job2"
./run.sh list
./run.sh history
```

- ‚úÖ Interface interativa e modo direto
- ‚úÖ Execu√ß√£o de jobs √∫nicos e m√∫ltiplos
- ‚úÖ Op√ß√µes avan√ßadas (limite, dry-run)
- ‚úÖ Hist√≥rico e inspe√ß√£o de banco

### üß™ `test.sh` - Testes e Valida√ß√£o

```bash
# Modo interativo
./test.sh

# Modo direto
./test.sh all
./test.sh imports
./test.sh cli
```

- ‚úÖ Teste de importa√ß√µes e funcionalidades
- ‚úÖ Valida√ß√£o de configura√ß√£o
- ‚úÖ Testes unit√°rios
- ‚úÖ Relat√≥rio de resumo

### üìñ Documenta√ß√£o dos Scripts

Para informa√ß√µes detalhadas sobre os scripts, consulte [SCRIPTS.md](SCRIPTS.md).

## üêõ Solu√ß√£o de Problemas

### Usando Scripts Shell (Recomendado)

```bash
# Executar testes completos
./test.sh all

# Verificar importa√ß√µes
./test.sh imports

# Testar CLI
./test.sh cli

# Verificar configura√ß√£o
./test.sh config
```

### Erro de Conex√£o

```bash
# Via scripts (mais f√°cil)
./run.sh list
./test.sh config

# Via CLI tradicional
data-runner list-jobs

# Testar conex√£o espec√≠fica
python -c "from app.connections import ConnectionFactory; from app.types import Connection, ConnectionType, ConnectionParams; conn = Connection('test', ConnectionType.SQLITE, ConnectionParams(filepath='./test.db')); db_conn = ConnectionFactory.create_connection(conn); print(db_conn.test_connection())"
```

### Erro de SQL

```bash
# Via scripts (mais f√°cil)
./run.sh run "problematic_job" --dry-run

# Via CLI tradicional
data-runner run --id problematic_job --dry-run

# Verificar logs detalhados
python -m app run --id problematic_job --verbose
```

### Problemas com DuckDB

```bash
# Verificar se o diret√≥rio data existe
ls -la data/

# Inspecionar tabelas criadas
python -m app inspect

# Verificar hist√≥rico de execu√ß√µes
python -m app history
```

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## üÜò Suporte

- **Issues**: [GitHub Issues](https://github.com/data-runner/data-runner/issues)
- **Discuss√µes**: [GitHub Discussions](https://github.com/data-runner/data-runner/discussions)

---

## üéØ Resumo das Funcionalidades

O **Data-Runner** √© uma ferramenta completa para processamento de dados que oferece:

### üîß **Funcionalidades Core**

- ‚úÖ **6 tipos de conex√£o**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- ‚úÖ **Sistema de vari√°veis**: Configura√ß√£o flex√≠vel com tipos (string, number, boolean)
- ‚úÖ **Sistema de depend√™ncias**: Jobs com depend√™ncias e execu√ß√£o ordenada
- ‚úÖ **Suporte a Schema**: Configura√ß√£o de schema padr√£o por conex√£o
- ‚úÖ **Oracle TNS Names**: Conex√µes Oracle via TNS Names (padr√£o corporativo)
- ‚úÖ **Conex√µes CSV**: Leitura direta de arquivos CSV sem SQL
- ‚úÖ **Valida√ß√£o robusta**: Detec√ß√£o de ciclos e depend√™ncias inv√°lidas
- ‚úÖ **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente
- ‚úÖ **Auditoria completa**: Rastreamento de todas as execu√ß√µes
- ‚úÖ **CLI intuitiva**: Interface de linha de comando f√°cil de usar
- ‚úÖ **Scripts Shell**: Instala√ß√£o, setup, execu√ß√£o e testes automatizados

### üöÄ **Casos de Uso**

- **ETL/ELT**: Pipelines de dados com depend√™ncias
- **Data Warehousing**: Carregamento de dados de m√∫ltiplas fontes
- **Integra√ß√£o de Sistemas**: Conex√£o com sistemas legados
- **An√°lise de Dados**: Processamento e transforma√ß√£o de dados
- **Automa√ß√£o**: Jobs agendados e parametrizados

**Data-Runner** - A ferramenta definitiva para migra√ß√£o e processamento de dados! üöÄ
