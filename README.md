# Data-Runner

üéØ **Executor de consultas/processos parametrizado por JSON**

Data-Runner √© uma ferramenta Python avan√ßada que permite executar consultas SQL em diferentes bancos de dados de forma parametrizada atrav√©s de arquivos JSON de configura√ß√£o. Com suporte a vari√°veis configur√°veis, sistema de depend√™ncias entre jobs, conex√µes Oracle via TNS Names, leitura de arquivos CSV e muito mais. Os resultados s√£o persistidos em um banco DuckDB local para an√°lise posterior.

## ‚ú® Caracter√≠sticas

- **Configura√ß√£o via JSON**: Defina conex√µes e jobs atrav√©s de arquivos JSON simples
- **M√∫ltiplos bancos suportados**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- **Persist√™ncia DuckDB**: Todos os resultados salvos localmente para an√°lise
- **Tipos de job**: Carga (dados) e Batimento (valida√ß√£o)
- **Auditoria completa**: Rastreamento de todas as execu√ß√µes
- **CLI intuitiva**: Interface de linha de comando f√°cil de usar
- **SQL parametrizado**: Suporte a vari√°veis de ambiente no SQL
- **Sistema de Vari√°veis**: Vari√°veis configur√°veis com tipos (string, number, boolean)
- **Sistema de Depend√™ncias**: Jobs com depend√™ncias e execu√ß√£o ordenada
- **Conex√µes CSV**: Leitura direta de arquivos CSV
- **Oracle TNS Names**: Suporte a conex√µes Oracle via TNS Names
- **Suporte a Schema**: Configura√ß√£o de schema padr√£o por conex√£o
- **Valida√ß√£o de Ciclos**: Detec√ß√£o autom√°tica de depend√™ncias circulares
- **Execu√ß√£o Paralela**: Jobs independentes executados em paralelo
- **Test√°vel e idempotente**: Execu√ß√µes seguras e repet√≠veis

## üöÄ Funcionalidades Principais

### üîó Sistema de Conex√µes

- **6 tipos de conex√£o**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- **Oracle TNS Names**: Suporte completo a conex√µes Oracle via TNS Names
- **Schema configur√°vel**: Defina schema padr√£o por conex√£o
- **Teste de conex√£o**: Valida√ß√£o autom√°tica de conex√µes

### üìä Sistema de Vari√°veis

- **Tipos suportados**: String, Number, Boolean
- **Substitui√ß√£o autom√°tica**: Vari√°veis processadas em tempo de execu√ß√£o
- **Valida√ß√£o de tipos**: Verifica√ß√£o autom√°tica de tipos de dados
- **Sintaxe simples**: Use `${var:nome_da_variavel}` em suas queries

### üîÑ Sistema de Depend√™ncias

- **Depend√™ncias entre jobs**: Configure jobs que dependem de outros
- **Ordena√ß√£o autom√°tica**: Execu√ß√£o na ordem correta baseada em depend√™ncias
- **Detec√ß√£o de ciclos**: Valida√ß√£o autom√°tica de depend√™ncias circulares
- **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente

### üìÅ Conex√µes CSV

- **Leitura direta**: Carregue dados de arquivos CSV sem SQL
- **Configura√ß√£o flex√≠vel**: Separador, encoding, header configur√°veis
- **Integra√ß√£o completa**: Funciona com sistema de depend√™ncias e vari√°veis

### üõ°Ô∏è Seguran√ßa e Configura√ß√£o

- **Arquivos de exemplo**: Templates seguros para configura√ß√£o
- **Gitignore autom√°tico**: Prote√ß√£o de dados sens√≠veis
- **Valida√ß√£o robusta**: Verifica√ß√£o de configura√ß√µes antes da execu√ß√£o

## üöÄ Instala√ß√£o

### Pr√©-requisitos

- Python 3.11 ou superior
- pip (gerenciador de pacotes Python)

### Instala√ß√£o b√°sica

```bash
# Clone o reposit√≥rio
git clone <repository-url>
cd data-runner

# Instale as depend√™ncias
pip install -e .

# Ou instale com depend√™ncias opcionais
pip install -e ".[all]"

# Configure os arquivos de configura√ß√£o
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json

# Edite os arquivos com suas configura√ß√µes reais
# config/connections.json - conex√µes de banco de dados
# config/jobs.json - jobs e queries
```

### Depend√™ncias opcionais

Para conectar com bancos espec√≠ficos, instale as depend√™ncias correspondentes:

```bash
# Para MySQL
pip install -e ".[mysql]"

# Para MSSQL
pip install -e ".[mssql]"

# Para Oracle
pip install -e ".[oracle]"

# Para desenvolvimento (inclui testes)
pip install -e ".[dev]"
```

## ‚ö° Quick Start

### 1. Configura√ß√£o B√°sica

```bash
# Copie os arquivos de exemplo
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json

# Edite com suas configura√ß√µes reais
```

### 2. Configura√ß√£o de Exemplos

Os exemplos de uso est√£o documentados na se√ß√£o "Funcionalidades Avan√ßadas" abaixo.

### 3. Execu√ß√£o de Jobs

```bash
# Executar job √∫nico
migradata run-job "load_people"

# Executar m√∫ltiplos jobs com depend√™ncias
migradata run-jobs "load_products_csv,transform_products,create_sales_fact"

# Executar todos os jobs
migradata run-all
```

## üìÅ Estrutura do Projeto

```
migradata/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                    # Interface de linha de comando
‚îÇ   ‚îú‚îÄ‚îÄ connections.py            # Gerenciamento de conex√µes
‚îÇ   ‚îú‚îÄ‚îÄ dependency_manager.py     # Gerenciamento de depend√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ repository.py             # Persist√™ncia DuckDB
‚îÇ   ‚îú‚îÄ‚îÄ runner.py                # Orquestrador principal
‚îÇ   ‚îú‚îÄ‚îÄ sql_utils.py             # Utilit√°rios SQL
‚îÇ   ‚îú‚îÄ‚îÄ types.py                 # Tipos e contratos
‚îÇ   ‚îî‚îÄ‚îÄ variable_processor.py    # Processamento de vari√°veis
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ connections.json.example  # Exemplo de conex√µes
‚îÇ   ‚îú‚îÄ‚îÄ jobs.json.example        # Exemplo de jobs
‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Documenta√ß√£o de configura√ß√£o
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ products.csv             # Dados de exemplo
‚îÇ   ‚îú‚îÄ‚îÄ customers.csv            # Dados de exemplo
‚îÇ   ‚îú‚îÄ‚îÄ sales_data.csv           # Dados de exemplo
‚îÇ   ‚îî‚îÄ‚îÄ warehouse.duckdb         # Banco DuckDB (criado automaticamente)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_config_parsing.py
‚îÇ   ‚îî‚îÄ‚îÄ test_runner_sqlite.py
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

## ‚öôÔ∏è Configura√ß√£o

### Configura√ß√£o Inicial

1. **Copie os arquivos de exemplo:**

```bash
cp config/connections.json.example config/connections.json
cp config/jobs.json.example config/jobs.json
```

2. **Edite as configura√ß√µes:**

   - `config/connections.json` - Configure suas conex√µes de banco
   - `config/jobs.json` - Configure seus jobs e queries

3. **Seguran√ßa:**
   - Os arquivos reais s√£o ignorados pelo Git
   - Use os arquivos `.example` como refer√™ncia
   - Nunca commite senhas ou dados sens√≠veis

### 1. Configura√ß√£o de Conex√µes (`config/connections.json`)

```json
{
  "defaultDuckDbPath": "./data/warehouse.duckdb",
  "connections": [
    {
      "name": "pg_main",
      "type": "postgres",
      "params": {
        "host": "localhost",
        "port": 5432,
        "database": "mydb",
        "user": "user",
        "password": "pass",
        "schema": "public"
      }
    },
    {
      "name": "mysql_analytics",
      "type": "mysql",
      "params": {
        "host": "localhost",
        "port": 3306,
        "database": "analytics",
        "user": "analytics_user",
        "password": "analytics_pass",
        "schema": "analytics"
      }
    },
    {
      "name": "mssql_warehouse",
      "type": "mssql",
      "params": {
        "host": "localhost",
        "port": 1433,
        "database": "warehouse",
        "user": "warehouse_user",
        "password": "warehouse_pass",
        "schema": "dbo"
      }
    },
    {
      "name": "sqlite_dev",
      "type": "sqlite",
      "params": {
        "filepath": "./data/dev.sqlite"
      }
    }
  ]
}
```

#### Suporte a Schema

O Data-Runner agora suporta configura√ß√£o de schema padr√£o para cada conex√£o:

- **PostgreSQL**: Use `"schema": "nome_do_schema"` para definir o search_path
- **MySQL**: Use `"schema": "nome_do_database"` para definir o database padr√£o
- **MSSQL**: Use `"schema": "nome_do_schema"` para definir o schema padr√£o
- **SQLite**: N√£o requer configura√ß√£o de schema (usa o arquivo diretamente)

O schema configurado ser√° aplicado automaticamente a todas as queries executadas na conex√£o.

### 2. Configura√ß√£o de Jobs (`config/jobs.json`)

```json
{
  "variables": {
    "start_date": {
      "value": "2024-01-01",
      "type": "string",
      "description": "Data de in√≠cio para filtros"
    },
    "min_amount": {
      "value": 100.0,
      "type": "number",
      "description": "Valor m√≠nimo"
    },
    "active_only": {
      "value": true,
      "type": "boolean",
      "description": "Filtrar apenas ativos"
    }
  },
  "jobs": [
    {
      "queryId": "orders_full_load",
      "type": "carga",
      "connection": "pg_main",
      "sql": "SELECT * FROM public.orders WHERE order_date >= '${var:start_date}' AND amount >= ${var:min_amount};",
      "targetTable": "stg_orders"
    },
    {
      "queryId": "customers_mismatch",
      "type": "batimento",
      "connection": "sqlite_dev",
      "sql": "SELECT c.id, c.email FROM customers c LEFT JOIN crm_customers crm ON crm.id = c.id WHERE crm.id IS NULL AND ${var:active_only} = true;"
    }
  ]
}
```

#### Sistema de Vari√°veis

O Data-Runner suporta vari√°veis configur√°veis que podem ser usadas em qualquer query:

**Tipos de Vari√°veis:**

- **`string`**: Texto (ser√° automaticamente envolvido em aspas simples)
- **`number`**: N√∫meros (inteiros ou decimais)
- **`boolean`**: Valores booleanos (true/false)

**Sintaxe de Uso:**

- Use `${var:nome_variavel}` nas suas queries SQL
- As vari√°veis s√£o substitu√≠das automaticamente durante a execu√ß√£o

**Exemplos:**

```sql
-- Vari√°vel string
SELECT * FROM users WHERE name = '${var:user_name}';

-- Vari√°vel num√©rica
SELECT * FROM orders WHERE amount >= ${var:min_amount};

-- Vari√°vel booleana
SELECT * FROM products WHERE active = ${var:active_only};
```

### Tipos de Conex√£o Suportados

- **PostgreSQL**: `"type": "postgres"`
- **SQLite**: `"type": "sqlite"`
- **MySQL**: `"type": "mysql"`
- **MSSQL**: `"type": "mssql"`
- **Oracle**: `"type": "oracle"`
- **CSV**: `"type": "csv"`

#### Conex√µes Oracle

O Data-Runner suporta conex√µes Oracle usando cx_Oracle com dois m√©todos:

##### 1. Conex√£o Direta (Host/Porta)

```json
{
  "name": "oracle_erp",
  "type": "oracle",
  "params": {
    "host": "oracle-server.example.com",
    "port": 1521,
    "database": "XE",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

##### 2. Conex√£o via TNS Names

```json
{
  "name": "oracle_tns_erp",
  "type": "oracle",
  "params": {
    "database": "ERP_PROD",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

**Par√¢metros Oracle:**

- `host`: Servidor Oracle (opcional para TNS Names)
- `port`: Porta do Oracle (opcional para TNS Names)
- `database`: Service Name, SID ou TNS Name do Oracle
- `user`: Usu√°rio Oracle
- `password`: Senha Oracle
- `schema`: Schema padr√£o (opcional)

**Conex√£o via TNS Names:**

- Configure o arquivo `tnsnames.ora` no cliente Oracle
- Use apenas `database` com o nome do TNS
- O `host` e `port` s√£o opcionais quando usando TNS Names

**Depend√™ncias:**

```bash
pip install cx_Oracle
```

#### Conex√µes CSV

O Data-Runner suporta leitura direta de arquivos CSV:

```json
{
  "name": "csv_products",
  "type": "csv",
  "params": {
    "csv_file": "./data/products.csv",
    "csv_separator": ",",
    "csv_encoding": "utf-8",
    "csv_has_header": true
  }
}
```

**Par√¢metros CSV:**

- `csv_file`: Caminho para o arquivo CSV (obrigat√≥rio)
- `csv_separator`: Separador usado no CSV (padr√£o: `,`)
- `csv_encoding`: Encoding do arquivo (padr√£o: `utf-8`)
- `csv_has_header`: Se o arquivo tem cabe√ßalho (padr√£o: `true`)

**Jobs CSV:**
Para conex√µes CSV, o campo `sql` √© opcional, pois o sistema l√™ o arquivo inteiro:

```json
{
  "queryId": "load_products_csv",
  "type": "carga",
  "connection": "csv_products",
  "targetTable": "stg_products"
}
```

#### Sistema de Depend√™ncias

O Data-Runner suporta depend√™ncias entre jobs para criar pipelines de dados:

```json
{
  "queryId": "transform_products",
  "type": "carga",
  "connection": "sqlite_dev",
  "sql": "SELECT * FROM stg_products WHERE active = 1;",
  "targetTable": "dim_products",
  "dependencies": ["load_products_csv"]
},
{
  "queryId": "create_sales_fact",
  "type": "carga",
  "connection": "sqlite_dev",
  "sql": "SELECT s.*, p.name FROM stg_sales s JOIN stg_products p ON s.product_id = p.id;",
  "targetTable": "fact_sales",
  "dependencies": ["load_sales_csv", "load_products_csv"]
}
```

**Funcionalidades:**

- **Ordena√ß√£o Autom√°tica**: Jobs s√£o executados na ordem correta das depend√™ncias
- **Valida√ß√£o de Ciclos**: Detecta depend√™ncias circulares
- **Execu√ß√£o Paralela**: Jobs sem depend√™ncias podem executar em paralelo
- **Resolu√ß√£o Inteligente**: Inclui automaticamente depend√™ncias necess√°rias

### Tipos de Job

- **Carga** (`"type": "carga"`): Carrega dados para tabela de staging
- **Batimento** (`"type": "batimento"`): Valida dados e salva diverg√™ncias

## üñ•Ô∏è Uso

### Comandos Principais

```bash
# Listar jobs dispon√≠veis
python -m app list

# Executar um job espec√≠fico
python -m app run --id orders_full_load

# Executar m√∫ltiplos jobs
python -m app run --ids orders_full_load,customers_mismatch

# Executar com op√ß√µes
python -m app run --id orders_full_load --limit 1000 --dry-run

# Ver hist√≥rico de execu√ß√µes
python -m app history

# Inspecionar tabelas no DuckDB
python -m app inspect
```

### Op√ß√µes de Execu√ß√£o

- `--duckdb <path>`: Caminho personalizado para o DuckDB
- `--dry-run`: Mostra o que faria sem executar
- `--limit <N>`: Limita o n√∫mero de linhas retornadas
- `--save-as <table>`: Nome personalizado para a tabela alvo

### Exemplos de Uso

```bash
# Execu√ß√£o simples
python -m app run --id orders_full_load

# Execu√ß√£o com limite e dry-run
python -m app run --id customers_mismatch --limit 100 --dry-run

# Execu√ß√£o em lote
python -m app run --ids orders_full_load,customers_mismatch,products_daily_sync

# Ver hist√≥rico de um job espec√≠fico
python -m app history --query-id orders_full_load

# Inspecionar tabela espec√≠fica
python -m app inspect --table stg_orders
```

### Uso Program√°tico com Schema

```python
from app.connections import create_connection_with_schema
from app.repository import ConfigRepository

# Carregar configura√ß√µes
repo = ConfigRepository("./config")
connections_config = repo.load_connections()

# Criar conex√£o com schema espec√≠fico
connection = next(c for c in connections_config.connections if c.name == "pg_main")
db_conn = create_connection_with_schema(connection)

# Executar query (schema ser√° aplicado automaticamente)
result = db_conn.execute_query("SELECT * FROM my_table")

# Mudar schema dinamicamente
db_conn.set_schema("other_schema")
result = db_conn.execute_query("SELECT * FROM other_table")

# Verificar schema atual
current_schema = db_conn.get_schema()
print(f"Schema atual: {current_schema}")

# Fechar conex√£o
db_conn.close()
```

### Uso Program√°tico com Vari√°veis

```python
from app.runner import JobRunner
from app.types import Variable, VariableType

# Inicializar runner
runner = JobRunner("./config")
runner.load_configs()

# Listar vari√°veis dispon√≠veis
variables = runner.list_variables()
print("Vari√°veis:", variables)

# Validar vari√°veis
errors = runner.validate_variables()
if errors:
    print("Erros:", errors)

# Adicionar nova vari√°vel dinamicamente
new_var = Variable(
    name="dynamic_param",
    value="Valor Din√¢mico",
    type=VariableType.STRING,
    description="Par√¢metro adicionado em runtime"
)
runner.add_variable(new_var)

# Executar job (vari√°veis ser√£o processadas automaticamente)
result = runner.run_job("my_job")
```

### Uso Program√°tico com Depend√™ncias

```python
from app.runner import JobRunner

# Inicializar runner
runner = JobRunner("./config")
runner.load_configs()

# Validar depend√™ncias
errors = runner.validate_dependencies()
if errors:
    print("Erros:", errors)

# Detectar ciclos
cycles = runner.detect_cycles()
if cycles:
    print("Ciclos:", cycles)

# Obter ordem de execu√ß√£o
execution_order = runner.get_execution_order()
print("Ordem:", execution_order)

# Obter grupos de execu√ß√£o paralela
execution_groups = runner.get_execution_groups()
print("Grupos:", execution_groups)

# Executar jobs respeitando depend√™ncias
results = runner.run_jobs(["job1", "job2", "job3"])

# Analisar depend√™ncias espec√≠ficas
deps = runner.get_job_dependencies("job2")
dependents = runner.get_dependent_jobs("job2")
print(f"Depend√™ncias de job2: {deps}")
print(f"Jobs que dependem de job2: {dependents}")
```

## üóÑÔ∏è Estrutura do DuckDB

### Tabela de Auditoria (`audit_job_runs`)

| Coluna         | Tipo    | Descri√ß√£o                     |
| -------------- | ------- | ----------------------------- |
| `run_id`       | VARCHAR | ID √∫nico da execu√ß√£o (UUID)   |
| `query_id`     | VARCHAR | ID da query executada         |
| `type`         | VARCHAR | Tipo do job (carga/batimento) |
| `started_at`   | VARCHAR | Timestamp de in√≠cio           |
| `finished_at`  | VARCHAR | Timestamp de fim              |
| `status`       | VARCHAR | Status (success/error)        |
| `rowcount`     | INTEGER | N√∫mero de linhas processadas  |
| `error`        | VARCHAR | Mensagem de erro (se houver)  |
| `target_table` | VARCHAR | Tabela alvo                   |
| `connection`   | VARCHAR | Conex√£o utilizada             |

### Conven√ß√µes de Nomenclatura

- **Jobs de Carga**: `stg_<queryId>` (ex: `stg_orders`)
- **Jobs de Batimento**: `val_<queryId>` (ex: `val_customers_mismatch`)

## üîß SQL Parametrizado

### Vari√°veis de Ambiente

Use `${env:VAR_NAME}` no SQL para referenciar vari√°veis de ambiente:

```sql
SELECT '${env:USER}' as current_user,
       '${env:HOME}' as home_dir,
       datetime('now') as current_time;
```

### Aplica√ß√£o de LIMIT

O par√¢metro `--limit` √© aplicado automaticamente ao SQL:

```bash
python -m app run --id orders_full_load --limit 1000
# Adiciona "LIMIT 1000" ao final da query
```

## üß™ Testes

Execute os testes com pytest:

```bash
# Instalar depend√™ncias de desenvolvimento
pip install -e ".[dev]"

# Executar todos os testes
pytest

# Executar com cobertura
pytest --cov=app

# Executar testes espec√≠ficos
pytest tests/test_config_parsing.py
pytest tests/test_runner_sqlite.py
```

## üìù Boas Pr√°ticas

### Para Jobs de Carga

1. **Use filtros de data**: Sempre inclua filtros de data para evitar carregar dados desnecess√°rios
2. **Defina target_table**: Especifique explicitamente o nome da tabela alvo
3. **Teste com --dry-run**: Use `--dry-run` antes de executar jobs grandes
4. **Use LIMIT para testes**: Use `--limit` para testar com poucos dados

```json
{
  "queryId": "orders_daily_load",
  "type": "carga",
  "connection": "pg_main",
  "sql": "SELECT * FROM orders WHERE order_date = CURRENT_DATE;",
  "targetTable": "stg_orders_daily"
}
```

### Para Jobs de Batimento

1. **Identifique diverg√™ncias claramente**: Use LEFT JOIN para encontrar registros √≥rf√£os
2. **Inclua campos identificadores**: Sempre inclua IDs para facilitar corre√ß√£o
3. **Documente a regra**: Use coment√°rios no SQL para explicar a valida√ß√£o

```json
{
  "queryId": "customers_orphan_validation",
  "type": "batimento",
  "connection": "sqlite_dev",
  "sql": "-- Valida√ß√£o: Clientes sem pedidos nos √∫ltimos 30 dias\nSELECT c.id, c.name, c.email, c.created_at\nFROM customers c\nLEFT JOIN orders o ON o.customer_id = c.id AND o.order_date >= date('now', '-30 days')\nWHERE o.id IS NULL;"
}
```

### Para Configura√ß√£o de Conex√µes

1. **Use vari√°veis de ambiente**: Para senhas e dados sens√≠veis
2. **Teste conex√µes**: Use `python -m app list` para validar configura√ß√µes
3. **Organize por ambiente**: Diferentes conex√µes para dev/prod

```json
{
  "name": "pg_prod",
  "type": "postgres",
  "params": {
    "host": "${env:PG_HOST}",
    "port": 5432,
    "database": "${env:PG_DATABASE}",
    "user": "${env:PG_USER}",
    "password": "${env:PG_PASSWORD}"
  }
}
```

## üöÄ Funcionalidades Avan√ßadas

### üìä Sistema de Vari√°veis Configur√°veis

O Data-Runner permite definir vari√°veis personalizadas que podem ser usadas em qualquer query SQL:

```json
{
  "variables": {
    "start_date": {
      "value": "2024-01-01",
      "type": "string",
      "description": "Data de in√≠cio para filtros"
    },
    "min_amount": {
      "value": 1000,
      "type": "number",
      "description": "Valor m√≠nimo para vendas"
    },
    "active_only": {
      "value": true,
      "type": "boolean",
      "description": "Filtrar apenas registros ativos"
    }
  }
}
```

**Uso nas queries:**

```sql
SELECT * FROM sales
WHERE date >= ${var:start_date}
  AND amount >= ${var:min_amount}
  AND active = ${var:active_only};
```

### üîÑ Sistema de Depend√™ncias Entre Jobs

Configure jobs que dependem de outros para criar pipelines de dados:

```json
{
  "jobs": [
    {
      "queryId": "load_products_csv",
      "type": "carga",
      "connection": "csv_products",
      "targetTable": "staging_products"
    },
    {
      "queryId": "transform_products",
      "type": "carga",
      "connection": "warehouse",
      "sql": "SELECT * FROM staging_products WHERE price > 0",
      "targetTable": "dim_products",
      "dependencies": ["load_products_csv"]
    },
    {
      "queryId": "create_sales_fact",
      "type": "carga",
      "connection": "warehouse",
      "sql": "SELECT p.*, s.* FROM dim_products p JOIN sales s ON p.id = s.product_id",
      "targetTable": "fact_sales",
      "dependencies": ["load_products_csv", "load_customers_csv"]
    }
  ]
}
```

**Funcionalidades:**

- ‚úÖ **Ordena√ß√£o autom√°tica**: Jobs executados na ordem correta
- ‚úÖ **Detec√ß√£o de ciclos**: Valida√ß√£o de depend√™ncias circulares
- ‚úÖ **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente
- ‚úÖ **Valida√ß√£o robusta**: Verifica√ß√£o de depend√™ncias inexistentes

### üìÅ Conex√µes CSV Avan√ßadas

Leia arquivos CSV diretamente sem necessidade de SQL:

```json
{
  "name": "csv_products",
  "type": "csv",
  "params": {
    "csv_file": "./data/products.csv",
    "csv_separator": ",",
    "csv_encoding": "utf-8",
    "csv_has_header": true
  }
}
```

**Jobs CSV:**

```json
{
  "queryId": "load_products_csv",
  "type": "carga",
  "connection": "csv_products",
  "targetTable": "staging_products"
}
```

### üîó Oracle TNS Names

Suporte completo a conex√µes Oracle via TNS Names:

```json
{
  "name": "oracle_erp",
  "type": "oracle",
  "params": {
    "database": "ERP_PROD",
    "user": "oracle_user",
    "password": "oracle_password",
    "schema": "HR"
  }
}
```

**Configura√ß√£o TNS Names:**

```bash
# Arquivo tnsnames.ora
ERP_PROD =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = oracle-server.company.com)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = ERP_PROD)
    )
  )
```

### üèóÔ∏è Suporte a Schema

Configure schema padr√£o para diferentes bancos:

```json
{
  "name": "postgres_analytics",
  "type": "postgres",
  "params": {
    "host": "localhost",
    "port": 5432,
    "database": "analytics",
    "user": "analytics_user",
    "password": "analytics_pass",
    "schema": "analytics"
  }
}
```

**Bancos suportados:**

- **PostgreSQL**: `search_path` configurado automaticamente
- **MySQL**: `USE schema` executado automaticamente
- **MSSQL**: `USE schema` executado automaticamente
- **Oracle**: `ALTER SESSION SET CURRENT_SCHEMA` executado automaticamente

## üêõ Solu√ß√£o de Problemas

### Erro de Conex√£o

```bash
# Verificar se a conex√£o est√° configurada corretamente
python -m app list

# Testar conex√£o espec√≠fica
python -c "from app.connections import ConnectionFactory; from app.types import Connection, ConnectionType, ConnectionParams; conn = Connection('test', ConnectionType.SQLITE, ConnectionParams(filepath='./test.db')); db_conn = ConnectionFactory.create_connection(conn); print(db_conn.test_connection())"
```

### Erro de SQL

```bash
# Executar com dry-run para ver o SQL processado
python -m app run --id problematic_job --dry-run

# Verificar logs detalhados
python -m app run --id problematic_job --verbose
```

### Problemas com DuckDB

```bash
# Verificar se o diret√≥rio data existe
ls -la data/

# Inspecionar tabelas criadas
python -m app inspect

# Verificar hist√≥rico de execu√ß√µes
python -m app history
```

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## üÜò Suporte

- **Issues**: [GitHub Issues](https://github.com/migradata/migradata/issues)
- **Discuss√µes**: [GitHub Discussions](https://github.com/migradata/migradata/discussions)

---

## üéØ Resumo das Funcionalidades

O **Data-Runner** √© uma ferramenta completa para processamento de dados que oferece:

### üîß **Funcionalidades Core**

- ‚úÖ **6 tipos de conex√£o**: PostgreSQL, SQLite, MySQL, MSSQL, Oracle, CSV
- ‚úÖ **Sistema de vari√°veis**: Configura√ß√£o flex√≠vel com tipos (string, number, boolean)
- ‚úÖ **Sistema de depend√™ncias**: Jobs com depend√™ncias e execu√ß√£o ordenada
- ‚úÖ **Suporte a Schema**: Configura√ß√£o de schema padr√£o por conex√£o
- ‚úÖ **Oracle TNS Names**: Conex√µes Oracle via TNS Names (padr√£o corporativo)
- ‚úÖ **Conex√µes CSV**: Leitura direta de arquivos CSV sem SQL
- ‚úÖ **Valida√ß√£o robusta**: Detec√ß√£o de ciclos e depend√™ncias inv√°lidas
- ‚úÖ **Execu√ß√£o paralela**: Jobs independentes executados simultaneamente
- ‚úÖ **Auditoria completa**: Rastreamento de todas as execu√ß√µes
- ‚úÖ **CLI intuitiva**: Interface de linha de comando f√°cil de usar

### üöÄ **Casos de Uso**

- **ETL/ELT**: Pipelines de dados com depend√™ncias
- **Data Warehousing**: Carregamento de dados de m√∫ltiplas fontes
- **Integra√ß√£o de Sistemas**: Conex√£o com sistemas legados
- **An√°lise de Dados**: Processamento e transforma√ß√£o de dados
- **Automa√ß√£o**: Jobs agendados e parametrizados

**MigraData** - A ferramenta definitiva para migra√ß√£o e processamento de dados! üöÄ
